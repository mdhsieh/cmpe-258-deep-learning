{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciwnWvkP6ndz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c803182-334a-4381-c214-2f9dac2327cd"
      },
      "source": [
        "'''\r\n",
        "********************************************************************\r\n",
        "* Program: hw1.py                                            \r\n",
        "* Coded by: Michael Hsieh                                                      \r\n",
        "* Date: Feb 12 2021\r\n",
        "*                                                                  \r\n",
        "* References: https://victorzhou.com/blog/intro-to-neural-networks/\r\n",
        "* Notes: \r\n",
        "* 1. A NN with one input layer, one hidden layer, and one output layer, \r\n",
        "* as 2-2-1 feed forward architecture.\r\n",
        "* 2. Initialized all biases and weights to 0.5\r\n",
        "* 3. The loss function was plotted with Matplotlib.\r\n",
        "* 4. The input data is predicted to belong to class 1 = C2.                            \r\n",
        "********************************************************************\r\n",
        "'''\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def sigmoid(x):\r\n",
        "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\r\n",
        "  return 1 / (1 + np.exp(-x))\r\n",
        "\r\n",
        "def deriv_sigmoid(x):\r\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\r\n",
        "  fx = sigmoid(x)\r\n",
        "  return fx * (1 - fx)\r\n",
        "\r\n",
        "def mse_loss(y_true, y_pred):\r\n",
        "  # y_true and y_pred are numpy arrays of the same length.\r\n",
        "  return ((y_true - y_pred) ** 2).mean()\r\n",
        "\r\n",
        "class OurNeuralNetwork:\r\n",
        "  '''\r\n",
        "  A neural network with:\r\n",
        "    - 2 inputs\r\n",
        "    - a hidden layer with 2 neurons (h1, h2)\r\n",
        "    - an output layer with 1 neuron (o1)\r\n",
        "  '''\r\n",
        "  def __init__(self):\r\n",
        "    # Weights\r\n",
        "    self.w1 = 0.5 # np.random.normal()\r\n",
        "    self.w2 = 0.5 # np.random.normal()\r\n",
        "    self.w3 = 0.5 # np.random.normal()\r\n",
        "    self.w4 = 0.5 # np.random.normal()\r\n",
        "    self.w5 = 0.5 # np.random.normal()\r\n",
        "    self.w6 = 0.5 # np.random.normal()\r\n",
        "\r\n",
        "    # Biases\r\n",
        "    self.b1 = 0.5 # np.random.normal()\r\n",
        "    self.b2 = 0.5 # np.random.normal()\r\n",
        "    self.b3 = 0.5 # np.random.normal()\r\n",
        "\r\n",
        "  def feedforward(self, x):\r\n",
        "    # x is a numpy array with 2 elements.\r\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\r\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\r\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\r\n",
        "    return o1\r\n",
        "\r\n",
        "  def train(self, data, all_y_trues):\r\n",
        "    '''\r\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\r\n",
        "    - all_y_trues is a numpy array with n elements.\r\n",
        "      Elements in all_y_trues correspond to those in data.\r\n",
        "    '''\r\n",
        "    learn_rate = 0.1\r\n",
        "    epochs = 1000 # number of times to loop through the entire dataset\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "      for x, y_true in zip(data, all_y_trues):\r\n",
        "        # --- Do a feedforward (we'll need these values later)\r\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\r\n",
        "        h1 = sigmoid(sum_h1)\r\n",
        "\r\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\r\n",
        "        h2 = sigmoid(sum_h2)\r\n",
        "\r\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\r\n",
        "        o1 = sigmoid(sum_o1)\r\n",
        "        y_pred = o1\r\n",
        "\r\n",
        "        # --- Calculate partial derivatives.\r\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\r\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\r\n",
        "\r\n",
        "        # Neuron o1\r\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\r\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\r\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\r\n",
        "\r\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\r\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\r\n",
        "\r\n",
        "        # Neuron h1\r\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\r\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\r\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\r\n",
        "\r\n",
        "        # Neuron h2\r\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\r\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\r\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\r\n",
        "\r\n",
        "        # --- Update weights and biases\r\n",
        "        # Neuron h1\r\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\r\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\r\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\r\n",
        "\r\n",
        "        # Neuron h2\r\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\r\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\r\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\r\n",
        "\r\n",
        "        # Neuron o1\r\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\r\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\r\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\r\n",
        "\r\n",
        "      # --- Calculate total loss at the end of each epoch\r\n",
        "      if epoch % 10 == 0:\r\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\r\n",
        "        loss = mse_loss(all_y_trues, y_preds)\r\n",
        "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\r\n",
        "\r\n",
        "        # Plot x = epoch and y = loss\r\n",
        "        # using blue circle markers\r\n",
        "        plt.plot(epoch, loss, \"bo\") \r\n",
        "        plt.xlabel(\"# Epochs\")\r\n",
        "        plt.ylabel(\"MSE Loss\")\r\n",
        "\r\n",
        "# Define dataset\r\n",
        "data = np.array([\r\n",
        "  [0.3, 0.457],  # C1\r\n",
        "  [1.1, 2.37],   # C1\r\n",
        "  [4.57, 5.55],   # C1\r\n",
        "  [0.5, 0.34], # C2\r\n",
        "  [1.45, 1.11], # C2\r\n",
        "  [4.78, 4.44], # C2\r\n",
        "])\r\n",
        "\r\n",
        "# 0 = class C1, 1 = class C2\r\n",
        "all_y_trues = np.array([\r\n",
        "  0, # C1\r\n",
        "  0, # C1\r\n",
        "  0, # C1\r\n",
        "  1, # C2\r\n",
        "  1, # C2\r\n",
        "  1 # C2\r\n",
        "])\r\n",
        "\r\n",
        "# Train our neural network\r\n",
        "network = OurNeuralNetwork()\r\n",
        "network.train(data, all_y_trues)\r\n",
        "\r\n",
        "# network = OurNeuralNetwork()\r\n",
        "# input data (3.32, 3.01), find which class, C1 or C2, input data belongs to\r\n",
        "x = np.array([3.32, 3.01])\r\n",
        "print(\"predicted class: %.3f\" % network.feedforward(x)) # 0.8663075199789684 - C2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 0.326\n",
            "Epoch 10 loss: 0.257\n",
            "Epoch 20 loss: 0.251\n",
            "Epoch 30 loss: 0.251\n",
            "Epoch 40 loss: 0.251\n",
            "Epoch 50 loss: 0.251\n",
            "Epoch 60 loss: 0.251\n",
            "Epoch 70 loss: 0.251\n",
            "Epoch 80 loss: 0.251\n",
            "Epoch 90 loss: 0.250\n",
            "Epoch 100 loss: 0.250\n",
            "Epoch 110 loss: 0.250\n",
            "Epoch 120 loss: 0.250\n",
            "Epoch 130 loss: 0.250\n",
            "Epoch 140 loss: 0.250\n",
            "Epoch 150 loss: 0.250\n",
            "Epoch 160 loss: 0.250\n",
            "Epoch 170 loss: 0.250\n",
            "Epoch 180 loss: 0.250\n",
            "Epoch 190 loss: 0.250\n",
            "Epoch 200 loss: 0.250\n",
            "Epoch 210 loss: 0.250\n",
            "Epoch 220 loss: 0.250\n",
            "Epoch 230 loss: 0.250\n",
            "Epoch 240 loss: 0.250\n",
            "Epoch 250 loss: 0.250\n",
            "Epoch 260 loss: 0.250\n",
            "Epoch 270 loss: 0.250\n",
            "Epoch 280 loss: 0.250\n",
            "Epoch 290 loss: 0.250\n",
            "Epoch 300 loss: 0.250\n",
            "Epoch 310 loss: 0.250\n",
            "Epoch 320 loss: 0.250\n",
            "Epoch 330 loss: 0.250\n",
            "Epoch 340 loss: 0.250\n",
            "Epoch 350 loss: 0.250\n",
            "Epoch 360 loss: 0.250\n",
            "Epoch 370 loss: 0.250\n",
            "Epoch 380 loss: 0.250\n",
            "Epoch 390 loss: 0.250\n",
            "Epoch 400 loss: 0.250\n",
            "Epoch 410 loss: 0.249\n",
            "Epoch 420 loss: 0.249\n",
            "Epoch 430 loss: 0.249\n",
            "Epoch 440 loss: 0.249\n",
            "Epoch 450 loss: 0.249\n",
            "Epoch 460 loss: 0.249\n",
            "Epoch 470 loss: 0.249\n",
            "Epoch 480 loss: 0.249\n",
            "Epoch 490 loss: 0.248\n",
            "Epoch 500 loss: 0.248\n",
            "Epoch 510 loss: 0.248\n",
            "Epoch 520 loss: 0.248\n",
            "Epoch 530 loss: 0.247\n",
            "Epoch 540 loss: 0.247\n",
            "Epoch 550 loss: 0.246\n",
            "Epoch 560 loss: 0.246\n",
            "Epoch 570 loss: 0.245\n",
            "Epoch 580 loss: 0.244\n",
            "Epoch 590 loss: 0.243\n",
            "Epoch 600 loss: 0.241\n",
            "Epoch 610 loss: 0.239\n",
            "Epoch 620 loss: 0.235\n",
            "Epoch 630 loss: 0.232\n",
            "Epoch 640 loss: 0.228\n",
            "Epoch 650 loss: 0.223\n",
            "Epoch 660 loss: 0.217\n",
            "Epoch 670 loss: 0.211\n",
            "Epoch 680 loss: 0.204\n",
            "Epoch 690 loss: 0.196\n",
            "Epoch 700 loss: 0.188\n",
            "Epoch 710 loss: 0.179\n",
            "Epoch 720 loss: 0.170\n",
            "Epoch 730 loss: 0.160\n",
            "Epoch 740 loss: 0.151\n",
            "Epoch 750 loss: 0.142\n",
            "Epoch 760 loss: 0.134\n",
            "Epoch 770 loss: 0.126\n",
            "Epoch 780 loss: 0.118\n",
            "Epoch 790 loss: 0.112\n",
            "Epoch 800 loss: 0.106\n",
            "Epoch 810 loss: 0.100\n",
            "Epoch 820 loss: 0.095\n",
            "Epoch 830 loss: 0.091\n",
            "Epoch 840 loss: 0.086\n",
            "Epoch 850 loss: 0.083\n",
            "Epoch 860 loss: 0.079\n",
            "Epoch 870 loss: 0.076\n",
            "Epoch 880 loss: 0.073\n",
            "Epoch 890 loss: 0.070\n",
            "Epoch 900 loss: 0.068\n",
            "Epoch 910 loss: 0.065\n",
            "Epoch 920 loss: 0.063\n",
            "Epoch 930 loss: 0.061\n",
            "Epoch 940 loss: 0.059\n",
            "Epoch 950 loss: 0.057\n",
            "Epoch 960 loss: 0.055\n",
            "Epoch 970 loss: 0.053\n",
            "Epoch 980 loss: 0.052\n",
            "Epoch 990 loss: 0.050\n",
            "predicted class: 0.866\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}